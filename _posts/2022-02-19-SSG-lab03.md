---
layout: single
title:  "[딥러닝기초] 04. forward pass & pytorch로 구현한 선형회귀"
categories: DL
tag: [python, deep learning, pytorch, linear regression]
toc: true
author_profile: false
---

<head>
  <style>
    table.dataframe {
      white-space: normal;
      width: 100%;
      height: 240px;
      display: block;
      overflow: auto;
      font-family: Arial, sans-serif;
      font-size: 0.9rem;
      line-height: 20px;
      text-align: center;
      border: 0px !important;
    }

    table.dataframe th {
      text-align: center;
      font-weight: bold;
      padding: 8px;
    }
    
    table.dataframe td {
      text-align: center;
      padding: 8px;
    }
    
    table.dataframe tr:hover {
      background: #b8d1f3; 
    }
    
    .output_prompt {
      overflow: auto;
      font-size: 0.9rem;
      line-height: 1.45;
      border-radius: 0.3rem;
      -webkit-overflow-scrolling: touch;
      padding: 0.8rem;
      margin-top: 0;
      margin-bottom: 15px;
      font: 1rem Consolas, "Liberation Mono", Menlo, Courier, monospace;
      color: $code-text-color;
      border: solid 1px $border-color;
      border-radius: 0.3rem;
      word-break: normal;
      white-space: pre;
    }

  .dataframe tbody tr th:only-of-type {
      vertical-align: middle;
  }

  .dataframe tbody tr th {
      vertical-align: top;
  }

  .dataframe thead th {
      text-align: center !important;
      padding: 8px;
  }

  .page__content p {
      margin: 0 0 0px !important;
  }

  .page__content p > strong {
    font-size: 0.8rem !important;
  }

  </style>
</head>


# Lab 3 – First Neural Network

- Work on saved a copy on your Drive, run ALL the code cells and answer the questions (code or text) before submitting the notebook.

- Try to visualise what each neural network model looks like.



## A. Implementing a basic "forward" pass



```python
import sys
import matplotlib as plt
import numpy as np
import torch
```

## The Perceptron (Single Neuron)

- Every non-input NEURON has a bias, not every weight, so the bias is added for a single output neuron

- 1 output neuron with 3 inputs



```python
# Initialise some values
input = [0.8, 1.3, 2.5]
weights = [2.5, 1.7, -0.5]
bias = 2

# Get the weigthed sum of the inputs and add the bias
output = input[0]*weights[0] + input[1]*weights[1] + input[2]*weights[2] + bias
print(output)

# Add a step function; output is 1 if it is 0 or more, 0 otherwise
output = 1 if output >= 0 else 0

# Print perceptron's final output
print(f'Final perceptron output = {output}') 

# TODO
# Q1. Think of a ML problem or task where a neural network would have a final output to be 0 or 1.
  ## ANSWER: Logistic regression for binary classification(class 0 or class 1), and image detection for find out whether the image has target object or not(0 is for FALSE, 1 is for TRUE).
```

<pre>
4.96
Final perceptron output = 1
</pre>
### Example 2: Multiple neurons

- Each neuron will have its own WEIGHT associated to each input

- As before, each NEURON will have its own BIAS



```python
#inputs = [0.8, 1.3, 2.5, 2.0] # A sample of 4 features, e.g. temperature, humidity, wind, pressure

input = [0.8, 1.3, 2.5, 2.0]

weights1 = [2.5, 1.7, -0.5, 1.0]
weights2 = [0.5, -0.91, 0.26, -0.5]
weights3 = [-0.26, -0.27, 0.17, 0.87]

bias1 = 2
bias2 = 3
bias3 = 0.5

output = [input[0]*weights1[0] + input[1]*weights1[1] + input[2]*weights1[2] + input[3]*weights1[3] + bias1,
          input[0]*weights2[0] + input[1]*weights2[1] + input[2]*weights2[2] + input[3]*weights2[3] + bias2,
          input[0]*weights3[0] + input[1]*weights3[1] + input[2]*weights3[2] + input[3]*weights3[3] + bias3]

print(output)

# TODO
# Q2. How many output neurons does this network contain?
  ## ANSWER: Thin network contains 3 output neurons(as there are 3 biases)
```

<pre>
[6.96, 1.867, 2.106]
</pre>

```python
torch.Tensor(output)
```

<pre>
tensor([6.9600, 1.8670, 2.1060])
</pre>
### Use Loops and Matrices!








```python
inputs = [0.8, 1.3, 2.5, 2.0]

weights = [[2.5, 1.7, -0.5, 1.0], 
           [0.5, -0.91, 0.26, -0.5],
           [-0.26, -0.27, 0.17, 0.87]]

biases = [2, 3, 0.5]

layer_outputs = [] # Outputs of current layer
for weight, bias in zip(weights, biases):
    output = 0 # Reset current neuron output to 0
    for input, w in zip(inputs, weight):
        output += input*w
    output += bias
    layer_outputs.append(output)

print(zip(weights, biases))
print(layer_outputs)
```

<pre>
<zip object at 0x7f0b3abba370>
[6.96, 1.867, 2.106]
</pre>
## Let's use NumPy & Dot Products!

- Multiply elements wise the corresponding values in two arrays and add them up

- Dot product between two vectors results in a single scalar value




```python
import numpy as np

inputs = np.array([0.8, 1.3, 2.5, 2.0])
weights = np.array([2.5, 1.7, -0.5, 1.0])
bias = 2

output = np.dot(weights, inputs) + bias
print(output)
```

<pre>
6.96
</pre>
When performing a dot product between a vector (input) and a matrix (weights)

  - Weights have to be first parameter of ``np.dot()``, otherwise an error will be thrown

  - Column size of ``weights`` = row size of ``inputs``




```python
inputs = np.array([0.8, 1.3, 2.5, 2.0])

weights = np.array([[2.5, 1.7, -0.5, 1.0], 
           [0.5, -0.91, 0.26, -0.5],
           [-0.26, -0.27, 0.17, 0.87]])

biases = np.array([2, 3, 0.5])

output = np.dot(weights, inputs) + biases # dot product of a vector and a matrix

print(output)

# TODO
# Q3. What else can we do to perform the dot product between the inputs and weights?
  # output = weights @ inputs + biases
  # output = np.dot(inputs, weights.T) + biases
```

<pre>
[6.96  1.867 2.106]
</pre>
## Summary

- Weights and biases are two different tools for approximating something (e.g. an output from an input)

- Weights determine how much influence this input has for the output (as it is multiplied) and bias determines how much this output can be offset by the input (as it is added)

- Think of a classical equation for a line ``y = wx + b`` where ``w`` is the weight and ``b`` is a the bias


## Dot product between two 2-D arrays (two matrices)

- Numpy's ``np.dot()`` is flexible; it computes the inner product for 1D arrays and performs matrix multiplication for 2D arrays.

- Since both matrices have the same size, transpose the second matrix, i.e. ``weights`` here so that ``1st-col-size=2nd-row-size`` holds 




```python
# inputs 3x4
inputs = np.array([[0.8, 1.3, 2.5, 2.0],
          [2.3, 1.5, 3.8, 4.5],
          [-1.5, 2.7, 3.3, -0.8]])

# weights 3x4
weights = np.array([[2.5, 1.7, -0.5, 1.0], 
           [0.5, -0.91, 0.26, -0.5],
           [-0.26, -0.27, 0.17, 0.87]])

biases = np.array([2, 3, 0.5])

output = np.dot(inputs, weights.T) + biases

print(f'{output} \n{output.shape}')

# TODO
# Q4. Explain what it means to have a 2-D array for the input and also for the output
  # 2-D array inputs mean they are multiple data. each input has multiple features(not just one scalar value)
  # Furthermore, 2-D ouputs correspond to inputs, so the ouputs refer to the combined and also extracted features came from inputs and give us the insight what to learn from the inputs.
```

<pre>
[[ 6.96   1.867  2.106]
 [12.9    1.523  4.058]
 [ 0.39   1.051  0.026]] 
(3, 3)
</pre>
## Let's use Tensors!

- ``torch.dot()`` behaves differently to ``np.dot()`` 

- ``torch.dot(a,b)`` treats both ``a`` and ``b`` as 1D vectors (irrespective of their original shape) and computes their inner product.

- Several ways to do matrix (rank 2 tensor) multiplication:

  - ``ab = a.mm(b)``

  - ``ab = torch.mm(a, b)``

  - ``ab = torch.matmul(a, b)``

  - ``ab = a @ b # Python 3.5+``

- The number of columns of the first matrix must be equal to the number of rows of the second matrix.

- The function ``torch.matmul()`` performs matrix multiplications if both arguments are 2D and computes their dot product if both arguments are 1D.




```python
# 1-D tensor dot product
a = torch.tensor([1, 2])
b = torch.tensor([3, 2])

torch.dot(a, b)
```

<pre>
tensor(7)
</pre>

```python
# 2-D tensor dot product for matrices with matching column and row size
a = torch.tensor([
    [0, 1, 2],
    [1, 2, 3]
])

b = torch.tensor([
    [0, 1],
    [1, 1],
    [1, 3]
])

torch.mm(a,b)
```

<pre>
tensor([[ 3,  7],
        [ 5, 12]])
</pre>
### Dot product between two 2-D tensors from the example



```python
input = torch.tensor(inputs)
weight = torch.tensor(weights)
bias = torch.tensor(biases)

output = torch.matmul(input, torch.transpose(weight, 0, 1)) + bias

print(output, output.shape)

# TODO
# Q5. repeat using the @ operator. You should get the same output
# output = weight @ input + bias
# output = input @ torch.transpose(weight, 0, 1) + bias
```

<pre>
tensor([[ 6.9600,  1.8670,  2.1060],
        [12.9000,  1.5230,  4.0580],
        [ 0.3900,  1.0510,  0.0260]], dtype=torch.float64) torch.Size([3, 3])
</pre>
### Exercise: Write your own code using TENSORS to implement a neural network as follows:

- Has 5 input neurons made up of random numbers between 0 and 1

- Connects to two output neurons

- Weights and biases should be random numbers between a normal distribution. Use `torch.randn()`

- The final outputs should be a combination of the weighted sum followed by an activation function as follows:

  - If the weighted sum is negative, then the output is 0

  - Otherwise the output is just the weighted sum




```python
# Q6. Has 5 input neurons made up of random numbers between 0 and 1. Use torch.rand()
inputs_ex = torch.rand(5)

# Q7. Weights should be random numbers between a normal distribution. Use torch.randn()
weights_ex1 = torch.randn(5, 1)
weights_ex2 = torch.randn(5, 1)

# Q8. Bias should be random numbers between a normal distribution. Use torch.randn()
bias_ex1 = torch.randn(1)
bias_ex2 = torch.randn(1)

# Q9. The final outputs should be a combination of the weighted sum
outputs_ex = torch.Tensor([torch.matmul(torch.transpose(weights_ex1, 0, 1), inputs_ex) + bias_ex1,
              torch.matmul(torch.transpose(weights_ex2, 0, 1), inputs_ex) + bias_ex2])
print(outputs_ex, outputs_ex.shape)

# Q10. If the weighted sum is negative, then the output is 0, otherwise the output is just the weighted sum

def nn_with_activation(input, weight1, weight2, bias1, bias2):
  output_ex = []
  output1 = torch.matmul(torch.transpose(weight1, 0, 1), input) + bias1
  output2 = torch.matmul(torch.transpose(weight2, 0, 1), input) + bias2
  if output1 >= 0:
    output_ex.append(float(output1))
  else:
    output_ex.append(0)
  if output2 >= 0:
    output_ex.append(float(output2))
  else:
    output_ex.append(0)
  output_ex = torch.tensor(output_ex)
  return output_ex

output1 = torch.matmul(inputs_ex, weights_ex1.T) + bias_ex1
output2 = torch.matmul(inputs_ex, weights_ex2.T) + bias_ex2
outputs = [output1, output2]
for i in range(outputs):
  if outputs[i] >= 0:
    outputs[i] = outputs[i]
  else:
    outputs[i] = 0
  return outputs


output_ex = nn_with_activation(inputs_ex, weights_ex1, weights_ex2, bias_ex1, bias_ex2)

# Print out the final output values
print(output_ex, output_ex.shape)
```

<pre>
tensor([-2.4454, -0.7729]) torch.Size([2])
tensor([0, 0]) torch.Size([2])
</pre>
## B. Build NN Model using ``torch.nn.Module``

- Create a subclass of nn.Module

- Define layers and activations

- Do a forward pass to inspect the model, parameters and outputs

- Add loss, optimiser and training loop for linear regression



```python
import torch

import torch.nn as nn

class TinyModel(nn.Module): # TinyModel is a subclass of nn.Module
    
    def __init__(self):
        super(TinyModel, self).__init__()
        super().__init__()
        self.linear1 = nn.Linear(100, 200) # input_size and hidden_size
        self.activation = nn.ReLU() # Rectified Linear Unit activation function
        self.linear2 = nn.Linear(200, 10) # hidden layer and output layer size
        self.softmax = nn.Softmax() # output layer activation function
    
    def forward(self, x):
        out = self.linear1(x)
        out = self.activation(out)
        out = self.linear2(out)
        out = self.softmax(out)
        return out

# Create an instance of TinyModel (like initalising a variable)
tinymodel = TinyModel()

# See what this model contains
print('The model:')
print(tinymodel)

# Print out one of the layers
print('\n\nJust one layer:')
print(tinymodel.linear2)

# View ALL the parameters (weights and biases)
# print('\n\nModel params:')
# for param in tinymodel.parameters():
#     print(param)

# View the parameters (weights and biases) in the LAST layer (linear2) only
print('\n\nLinear2 layer params:')
for param in tinymodel.linear2.parameters():
    print(param)
```

<pre>
The model:
TinyModel(
  (linear1): Linear(in_features=100, out_features=200, bias=True)
  (activation): ReLU()
  (linear2): Linear(in_features=200, out_features=10, bias=True)
  (softmax): Softmax(dim=None)
)


Just one layer:
Linear(in_features=200, out_features=10, bias=True)


Linear2 layer params:
Parameter containing:
tensor([[-0.0170, -0.0101, -0.0674,  ..., -0.0227,  0.0598,  0.0167],
        [ 0.0236, -0.0053, -0.0461,  ..., -0.0619, -0.0278, -0.0429],
        [ 0.0062, -0.0010, -0.0660,  ..., -0.0383,  0.0653, -0.0707],
        ...,
        [ 0.0413, -0.0678, -0.0697,  ...,  0.0177, -0.0175, -0.0504],
        [-0.0332, -0.0198,  0.0652,  ..., -0.0390, -0.0349,  0.0035],
        [ 0.0124,  0.0403, -0.0556,  ..., -0.0194,  0.0047, -0.0326]],
       requires_grad=True)
Parameter containing:
tensor([-0.0119,  0.0402, -0.0656, -0.0597, -0.0095, -0.0523,  0.0609,  0.0483,
        -0.0695,  0.0161], requires_grad=True)
</pre>
### Linear layer demo



For the code chunk below, note that:

- the weight, bias and output tensor dimensions are as expected (weight matrix is in transposed form, i.e. has shape (out_features, in_features)

- the multiplication of the inputs with the weight matrices, added with the bias will produce the output



```python
# Create a linear layer with 3 inputs and 2 outputs
lin = torch.nn.Linear(3, 2)
# Create sample input vector with random values
x = torch.rand(1, 3)
print('Input:')
print(x)

# Print the initial parameter values (these are also randomly set)
print('\n\nWeight and Bias parameters:')
for param in lin.parameters():
    print(param)

# Apply the linear layer to the input
y = lin(x)
print('\n\nOutput:')
print(y)
```

<pre>
Input:
tensor([[0.0137, 0.7802, 0.8769]])


Weight and Bias parameters:
Parameter containing:
tensor([[ 0.1970,  0.2316,  0.1302],
        [ 0.5642,  0.1836, -0.4587]], requires_grad=True)
Parameter containing:
tensor([0.5408, 0.4702], requires_grad=True)


Output:
tensor([[0.8384, 0.2190]], grad_fn=<AddmmBackward>)
</pre>
- Note that you see ``requires_grad`` for the parameters. This means that their values will be tracked and used for **gradient** calculation during the training phase.

- Although ``Parameter`` is a subclass of ``Tensor``, it has a special extended behaviour in that its gradients can be tracked for NN learning purposes.



### Add Training Loop, Loss and Optimiser

- A loop is necessary to allow NN training to happen iteratively.

- The number of times we want to repeat the cycle is called number of epochs.

- Loss functions tell us how far a model's prediction is from the correct answer. PyTorch contains a variety of loss functions, including common MSE (mean squared error = L2 norm), Cross Entropy Loss and Negative Likelihood Loss (useful for classifiers), and others.

- An optimiser helps to minimise the loss by taking iterative steps in the direction that minimises the loss by updating the parameters in each epoch. We will use Stochastic Gradient Descent (SGD) for this example.





## Example: Linear Regression

1. Provide training data. We will provide the inputs, ``x`` and outputs, ``y`` for the formula ``y = 3x + 2``. 

  - Remember to provide them as tensors. 

  - Note that ``y`` will have to be correct values that satisfy the formula.

2. Define the NN class. 

  - Since we will only take in a number and also spit out a number, we will have the ``input_size`` to be 1 and the ``output_size`` to be 1 as well. 

  - For linear regression, the output will be a real value, so no output layer activation function is needed.

  - Note also that an extra parameter ``hidden_size`` was added to the initialiser

3. Create an instance of the NN class

  - Pass in the required parameter ``hidden_size``

4. Define Loss and Optimiser

  - for this linear regression problem, we will use the MSE Loss and SGD optimiser

5. Training Loop

  - Loop through a set number of times, e.g. 100

    - forward pass to calculate the predicted values for each input sample and calculate the loss using the loss instance you created earlier

    - backward pass to first reset all the gradients ``opt.zero_grad()``, then to calculate the gradients of the loss w.r.t. the params ``loss.backward()`` and finally update the params accordingly ``opt.step()``



6. Test the model

  - At this point, the model has been trained, i.e. the parameters have been optimised and the loss should be small enough.

  - Call ``model()`` with any new number to test what the trained model will predict. See how accurately it performs.

  - Test with several other numbers.



### NOTES:

- Data will need to be prepared so that it can be used with PyTorch. Here we have specified the data as a tensor and also made sure that it is of type float. 

- It is good practice to specify hyper parameters separately, e.g. ``hidden_size``, ``learning_rate``, ``num_epochs``, etc.




```python
import torch

import torch.nn as nn


# 1. Define training data for a simple mathematical formula y = 3x + 2 for x=1, ..., 5
x = torch.tensor([[1],[2],[3],[4],[5]], dtype=torch.float32)
y = torch.tensor([[5],[8],[11],[14],[17]], dtype=torch.float32)

# 2. Define NN class 
class MyNN(nn.Module): # TinyModel is a subclass of nn.Module
    
    def __init__(self, hidden_size):
        super().__init__()
        self.linear1 = nn.Linear(1, hidden_size) # input_size and hidden_size
        self.activation = nn.ReLU() # Rectified Linear Unit activation function
        self.linear2 = nn.Linear(hidden_size, 1) # hidden layer and output layer size
        # output layer is a real value for regression, does not need any activation
    
    def forward(self, x):
        x = self.linear1(x)
        x = self.activation(x)
        x = self.linear2(x)
        return x

# 3. Create an instance of NN model and define the hidden_size
hidden_size = 4
model = MyNN(hidden_size) # now 'model' will be a callable function

# 4. Loss and Optimiser
learning_rate = 0.01
loss_fn = nn.MSELoss() # now 'loss_fn' will be a callable function
opt = torch.optim.SGD(model.parameters(), lr=learning_rate)

# 5. Training loop
num_epochs = 100

for epoch in range(num_epochs):
  # 5.1 Forward pass
  y_pred = model(x)
  loss = loss_fn(y_pred, y) 

  # 5.2 Backward pass
  opt.zero_grad() # Reset gradients at each epoch
  loss.backward() # Backpropagate the loss gradients w.r.t the params
  opt.step() # Update params

  # 5.3 Print loss every 10th epoch
  if (epoch+1) % 10 == 0:
    print(f'Epoch {epoch+1}/{num_epochs}, Loss={loss.item():.4f} ')
```

<pre>
Epoch 10/100, Loss=0.2017 
Epoch 20/100, Loss=0.1502 
Epoch 30/100, Loss=0.1131 
Epoch 40/100, Loss=0.0853 
Epoch 50/100, Loss=0.0645 
Epoch 60/100, Loss=0.0488 
Epoch 70/100, Loss=0.0370 
Epoch 80/100, Loss=0.0281 
Epoch 90/100, Loss=0.0213 
Epoch 100/100, Loss=0.0162 
</pre>

```python
# 6. Prediction
# Test the model on any new x
test_num = 10
test = torch.tensor([test_num], dtype=torch.float32)

# Get the model's prediction for this test number
print(f'\n\nModel prediction for {test_num} is {model(test).item():.4f}')
```

<pre>


Model prediction for 10 is 32.5225
</pre>

```python
# trial for Question 13.
num_epochs = 200

for epoch in range(num_epochs):
  y_pred = model(x)
  loss = loss_fn(y_pred, y) 

  opt.zero_grad()
  loss.backward()
  opt.step()

  if (epoch+1) % 10 == 0:
    print(f'Epoch {epoch+1}/{num_epochs}, Loss={loss.item():.4f} ')

test_num = 10
test = torch.tensor([test_num], dtype=torch.float32)

print(f'\n\nModel prediction for {test_num} is {model(test).item():.4f}')
```

<pre>
Epoch 10/200, Loss=0.0123 
Epoch 20/200, Loss=0.0094 
Epoch 30/200, Loss=0.0071 
Epoch 40/200, Loss=0.0054 
Epoch 50/200, Loss=0.0041 
Epoch 60/200, Loss=0.0031 
Epoch 70/200, Loss=0.0024 
Epoch 80/200, Loss=0.0018 
Epoch 90/200, Loss=0.0014 
Epoch 100/200, Loss=0.0011 
Epoch 110/200, Loss=0.0008 
Epoch 120/200, Loss=0.0006 
Epoch 130/200, Loss=0.0005 
Epoch 140/200, Loss=0.0004 
Epoch 150/200, Loss=0.0003 
Epoch 160/200, Loss=0.0002 
Epoch 170/200, Loss=0.0002 
Epoch 180/200, Loss=0.0001 
Epoch 190/200, Loss=0.0001 
Epoch 200/200, Loss=0.0001 


Model prediction for 10 is 32.0343
</pre>

```python
# trial for Question 14.
hidden_size = 8
model = MyNN(hidden_size)

learning_rate = 0.01
loss_fn = nn.MSELoss()
opt = torch.optim.SGD(model.parameters(), lr=learning_rate)

num_epochs = 100

for epoch in range(num_epochs):
  # 5.1 Forward pass
  y_pred = model(x)
  loss = loss_fn(y_pred, y) 

  opt.zero_grad()
  loss.backward()
  opt.step()

  if (epoch+1) % 10 == 0:
    print(f'Epoch {epoch+1}/{num_epochs}, Loss={loss.item():.4f} ')

test_num = 10
test = torch.tensor([test_num], dtype=torch.float32)

print(f'\n\nModel prediction for {test_num} is {model(test).item():.4f}')
```

<pre>
Epoch 10/100, Loss=0.0683 
Epoch 20/100, Loss=0.0469 
Epoch 30/100, Loss=0.0327 
Epoch 40/100, Loss=0.0227 
Epoch 50/100, Loss=0.0158 
Epoch 60/100, Loss=0.0110 
Epoch 70/100, Loss=0.0077 
Epoch 80/100, Loss=0.0054 
Epoch 90/100, Loss=0.0038 
Epoch 100/100, Loss=0.0026 


Model prediction for 10 is 32.2497
</pre>

```python
# TODO
# Q11. Did the loss decrease over time during training?
  # Loss got decreased as training epochs go by.

# Q12. Was the prediction of the model for x=10 close to the ideal value that it should have?
  # The ideal value is 32, and the prediction value is 31.3954.The ratio of residual among ideal value is about 1.9%(for instant trail) so I think it is close enough.

# Q13. Change the num_epochs to 200 and retrain the model. Is the prediction for x=10 better? Why?
  # The prediction got better as it is closer to the ideal value. The reason for that is normally as training complexity increases the loss decreases in training.

# Q14. Set num_epoch=100 and change the hidden_size to 8. Retrain the model. Is the model's performance better?
  # Loss got bigger compared to the previous trial that set epoch=200 and hidden_size=4.

# Q15. Try increasing or decreasing num_epoch and/or hidden_size to see which combination gives you the best model. 
# Keep the output of your code execution above for the best combination and state which combination of num_epoch 
# and hidden_size values gave you the best model.
  # hidden_size=5 / num_epoch=400 was the best.
```


```python
hidden_size = 5
model = MyNN(hidden_size)

learning_rate = 0.01
loss_fn = nn.MSELoss()
opt = torch.optim.SGD(model.parameters(), lr=learning_rate)

num_epochs = 400

for epoch in range(num_epochs):
  # 5.1 Forward pass
  y_pred = model(x)
  loss = loss_fn(y_pred, y) 

  opt.zero_grad()
  loss.backward()
  opt.step()

  if (epoch+1) % 10 == 0:
    print(f'Epoch {epoch+1}/{num_epochs}, Loss={loss.item():.4f} ')

test_num = 10
test = torch.tensor([test_num], dtype=torch.float32)

print(f'\n\nModel prediction for {test_num} is {model(test).item():.4f}')
```

<pre>
Epoch 10/400, Loss=0.0432 
Epoch 20/400, Loss=0.0285 
Epoch 30/400, Loss=0.0205 
Epoch 40/400, Loss=0.0147 
Epoch 50/400, Loss=0.0106 
Epoch 60/400, Loss=0.0076 
Epoch 70/400, Loss=0.0055 
Epoch 80/400, Loss=0.0039 
Epoch 90/400, Loss=0.0028 
Epoch 100/400, Loss=0.0020 
Epoch 110/400, Loss=0.0015 
Epoch 120/400, Loss=0.0010 
Epoch 130/400, Loss=0.0008 
Epoch 140/400, Loss=0.0005 
Epoch 150/400, Loss=0.0004 
Epoch 160/400, Loss=0.0003 
Epoch 170/400, Loss=0.0002 
Epoch 180/400, Loss=0.0001 
Epoch 190/400, Loss=0.0001 
Epoch 200/400, Loss=0.0001 
Epoch 210/400, Loss=0.0001 
Epoch 220/400, Loss=0.0000 
Epoch 230/400, Loss=0.0000 
Epoch 240/400, Loss=0.0000 
Epoch 250/400, Loss=0.0000 
Epoch 260/400, Loss=0.0000 
Epoch 270/400, Loss=0.0000 
Epoch 280/400, Loss=0.0000 
Epoch 290/400, Loss=0.0000 
Epoch 300/400, Loss=0.0000 
Epoch 310/400, Loss=0.0000 
Epoch 320/400, Loss=0.0000 
Epoch 330/400, Loss=0.0000 
Epoch 340/400, Loss=0.0000 
Epoch 350/400, Loss=0.0000 
Epoch 360/400, Loss=0.0000 
Epoch 370/400, Loss=0.0000 
Epoch 380/400, Loss=0.0000 
Epoch 390/400, Loss=0.0000 
Epoch 400/400, Loss=0.0000 


Model prediction for 10 is 32.0013
</pre>
### Let's plot the landscape of the MSE loss over a range of values for weight

- The sample training data has function ``y=3x+2``. However, this relationship is **NOT** known by the model. We only have ``x``'s and ``y``'s.

- So we try different values of weight, ``w``, between -1 and 5 to fit the equation ``y = wx + 2``

- When we do that, we calculate the MSE loss between the predicted outcome, ``pred`` and the actual outcome, ``y`` for each ``w`` that we use.

- The loss that occurred for each weight is plotted and connected.



```python
import numpy as np
import matplotlib.pyplot as plt

# Create a range of values from -1 to 5
w_range = np.linspace(-1, 5, 1000)
loss_w_range = [] # create an empty list to store the loss values

# Try different weight values to get predictions for y
for w in w_range:
    pred = (w * x) + 2
    loss = torch.mean((pred - y) ** 2)
    loss_w_range.append(loss.item())

# Plot the graph
plt.title("Loss Over Different Weight Values for the Linear Function y=3x+2")
plt.plot(w_range, loss_w_range)
plt.xlabel('Weight, w')
plt.ylabel('MSE Loss')
plt.show()
```

<img src="data:image/png;base64,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"/>


```python
# Q16. From the graph, what value of weight gives the minimum loss for this linear function?
  # 3 seems to give minimum loss for this model as the gradient of the graph converges to zero at 3 
```
